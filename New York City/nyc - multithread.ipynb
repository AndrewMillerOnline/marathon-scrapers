{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Scraper\n",
    "\n",
    "    Years: 2014 - 2019\n",
    "    URL: https://results.nyrr.org/event/M{year}/finishers\n",
    "    \n",
    "    Uses multithreading and Selenium to process 5,000 results per thread.  Warning, this will consume about 1 GB of RAM per worker, so set the number carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementNotVisibleException, ElementNotSelectableException, NoSuchElementException, TimeoutException, UnexpectedAlertPresentException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'C:/dev/chromedriver.exe'\n",
    "\n",
    "def find_page_limit(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--window-size=1920,1200\")\n",
    "    driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "\n",
    "    #Open the results page\n",
    "    driver.get(url)\n",
    "    \n",
    "    runner_count = driver.find_element_by_xpath(\"//ul[@class='submenu nav nav-tabs']//span[@class='ng-binding']\").text\n",
    "    return runner_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urls(limit, base_url):\n",
    "    \n",
    "    #There are 51 results per page.  To speed things up, we'll grab 100 pages of results per worker.\n",
    "        \n",
    "    i = 1\n",
    "    \n",
    "    while i <= limit:\n",
    "        pages_to_scrape.append(base_url + f'#page=1&opf={i}')\n",
    "        i += 5100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_results(url):\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--window-size=1920,1200\")\n",
    "    driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "\n",
    "    # Open the results page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Give the page some time to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Change to narrow view\n",
    "    driver.find_element_by_xpath(\"//span[@class='grid-row-btn narrow-row-icon']\").click()\n",
    "    \n",
    "    # We're opening 10+ pages all at once, so give it a little more time to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try_do_scrape(driver)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_do_scrape(driver):\n",
    "    \n",
    "    # Click the 'Load More Results' button 100 times, then grab all the results on the page\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        \n",
    "        try:\n",
    "            ## Last, find and click the next page button (using Keys.ENTER results in fewer errors than .click())\n",
    "            next_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='button-load-more']\")))\n",
    "\n",
    "            next_button.click();\n",
    "            \n",
    "            #wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='button-load-more']\")))\n",
    "            #next_button = driver.find_element_by_xpath(\"//a[@class='button-load-more']\")\n",
    "            #next_button.send_keys(Keys.ENTER)\n",
    "            \n",
    "            #next_button.click()\n",
    "\n",
    "            ### Wait for the next button to become stale, meaning we've successfully navigated to the next page of results\n",
    "            ### This will throw a TimeoutException on the last page of results, since that page has no next button\n",
    "            ### We'll just catch the exception once, rather than checking the button's existance on every single page\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            continue\n",
    "            \n",
    "    ## Send the table to parse_page\n",
    "    parse_page(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a page of marathon finishers, adding the results to the master list\n",
    "def parse_page(html):\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    page_finishers = soup.find_all('div','cmd-finisher ng-scope')\n",
    "\n",
    "    for finisher in page_finishers:\n",
    "        \n",
    "        name = finisher.find('div','name rms-grid-line ng-binding').get_text().strip()\n",
    "        \n",
    "        details = finisher.find('div', 'details rms-grid-line').find_all('span')\n",
    "        gender = details[0].get_text()[0:1]\n",
    "        age = details[0].get_text()[1:]\n",
    "        nationality = details[1].get_text()\n",
    "        bib_number = finisher.find('span', attrs={\"ng-if\": \"eventFinisher.bib\"}).get_text().strip(\"Bib \")\n",
    "        \n",
    "        time_full = finisher.find('span', 'result right-floated-item long-text').get_text()[4:]\n",
    "        place = finisher.find('span', 'result right-floated-item mid-text').get_text()[5:]\n",
    "\n",
    "        result = {\n",
    "            \"place\" : place,\n",
    "            \"name\" : name,\n",
    "            \"gender\" : gender,\n",
    "            \"age\" : age,\n",
    "            \"nationality\" : nationality,\n",
    "            \"bib_number\" : bib_number,\n",
    "            \"time_full\" : time_full\n",
    "        }\n",
    "\n",
    "        masterResults.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_save(year, limit):\n",
    "    df_results = pd.DataFrame(masterResults)\n",
    "    \n",
    "    # Dedupe, just in case we gathered duplicate pages\n",
    "    df_results = df_results.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Add the event name\n",
    "    df_results['Race'] = 'New York City'\n",
    "    df_results['Year'] = year\n",
    "    \n",
    "    # Save to csv\n",
    "    df_results.to_csv(f'results-{year}.csv', index=False)\n",
    "    print(f\"Scraping complete.  Gathered {df_results.shape[0]} of {limit} results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete.  Gathered 53520 of 53520 results.\n"
     ]
    }
   ],
   "source": [
    "###### SET YEAR HERE #######\n",
    "year = '2019'\n",
    "############################\n",
    "\n",
    "url = f'https://results.nyrr.org/event/M{year}/finishers'\n",
    "\n",
    "# find page limit\n",
    "limit = int(find_page_limit(url))\n",
    "\n",
    "# generate urls to scrape\n",
    "pages_to_scrape = []\n",
    "masterResults = []\n",
    "generate_urls(limit, url)\n",
    "\n",
    "# Scrape the urls\n",
    "\n",
    "## We'll use multithreading to speed up the scraping process significantly\n",
    "## However, performance seems to suffer when using more than 4 workers\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(scrape_results, pages_to_scrape)\n",
    "cleanup_and_save(year, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
