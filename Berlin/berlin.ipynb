{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Scraper\n",
    "\n",
    "    Years: 2009 (so far) - 2019\n",
    "    URL: https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementNotVisibleException, ElementNotSelectableException, NoSuchElementException, TimeoutException, UnexpectedAlertPresentException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a page of marathon finishers, adding the results to the master list\n",
    "def parse_page(html):\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    page_finishers = soup.find(\"table\").tbody.find_all(\"tr\")\n",
    "\n",
    "    for finisher in page_finishers:\n",
    "        \n",
    "        if not finisher.has_attr('class') or 'shown' not in finisher['class']:\n",
    "            split_list = finisher.find(\"ul\", \"splits\")\n",
    "            if split_list is None:\n",
    "                continue\n",
    "            splits = split_list.find_all('li')\n",
    "            \n",
    "            split_5k = splits[0].find_all('span')[1].get_text()\n",
    "            split_10k = splits[1].find_all('span')[1].get_text()\n",
    "            split_15k = splits[2].find_all('span')[1].get_text()\n",
    "            split_20k = splits[3].find_all('span')[1].get_text()\n",
    "            time_half = splits[4].find_all('span')[1].get_text()\n",
    "            split_25k = splits[5].find_all('span')[1].get_text()\n",
    "            split_30k = splits[6].find_all('span')[1].get_text()\n",
    "            split_35k = splits[7].find_all('span')[1].get_text()\n",
    "            split_40k = splits[8].find_all('span')[1].get_text()\n",
    "            \n",
    "            split_times = {\n",
    "                'split_5k' : split_5k,\n",
    "                'split_10k' : split_10k,\n",
    "                'split_15k' : split_15k,\n",
    "                'split_20k' : split_20k,\n",
    "                'time_half' : time_half,\n",
    "                'split_25k' : split_25k,\n",
    "                'split_30k' : split_30k,\n",
    "                'split_35k' : split_35k,\n",
    "                'split_40k' : split_40k\n",
    "            }\n",
    "            \n",
    "            if len(masterResults) > 0:\n",
    "                masterResults[len(masterResults)-1].update(split_times)\n",
    "            continue\n",
    "        \n",
    "        cells = finisher.find_all('td')\n",
    "        \n",
    "        #skip header row\n",
    "        if cells is None or len(cells) == 0:\n",
    "            continue\n",
    "\n",
    "        place_overall = cells[1].get_text()\n",
    "        \n",
    "        first_name = cells[2].get_text()\n",
    "        last_name = cells[3].get_text()\n",
    "        \n",
    "        nationality = cells[4].get_text()\n",
    "        club = cells[6].get_text()\n",
    "        gender = cells[7].get_text()\n",
    "        \n",
    "        time_full = cells[8].get_text()\n",
    "\n",
    "        result = {\n",
    "            \"place_overall\" : place_overall,\n",
    "            \"first_name\" : first_name,\n",
    "            \"last_name\" : last_name,\n",
    "            \"nationality\" : nationality,\n",
    "            \"club\" : club,\n",
    "            \"time_full\" : time_full,\n",
    "            \"gender\" : gender\n",
    "        }\n",
    "\n",
    "        masterResults.append(result)\n",
    "\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pagination_limit(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #The second to last item in the pagination object displays the final page number, that's what we want\n",
    "    pagination = soup.find(\"ul\", \"pagination\").find_all('li')\n",
    "    li_length = len(pagination)\n",
    "    last_page = int(pagination[li_length - 2].text)\n",
    "    \n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_save(year):\n",
    "    df_results = pd.DataFrame(masterResults)\n",
    "    \n",
    "    #add the event name\n",
    "    df_results['Race'] = 'Berlin'\n",
    "    \n",
    "    #add the year\n",
    "    df_results['year'] = year\n",
    "    \n",
    "    df_results.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    #save to csv\n",
    "    df_results.to_csv(f'results-{year}.csv', index=False)\n",
    "    print(f\"Scraping complete.  Gathered {df_results.shape[0]} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_do_scrape(current_page, last_page):\n",
    "    while current_page < last_page:\n",
    "\n",
    "        ### Wait for the 'loading' modal to go away\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, '//table/tbody/tr')))\n",
    "\n",
    "        current_page = int(driver.find_element_by_xpath(\"//li[@class='paginate_button page-item active']\").text)\n",
    "        print(f\"Page {current_page} of {last_page}: {len(masterResults)} results so far\")\n",
    "\n",
    "        ## First, \"click\" on each tr to expand it\n",
    "        \n",
    "        ### Grab the table rows\n",
    "        result_row = driver.find_element_by_xpath('//table/tbody').find_elements_by_tag_name('tr')\n",
    "\n",
    "        ### Click 'em all\n",
    "        for row in result_row:\n",
    "            row.click()\n",
    "\n",
    "        ## Second, send the table to parse_page\n",
    "        parse_page(driver.page_source)\n",
    "\n",
    "        ## Last, find and click the next page button (using Keys.ENTER results in fewer errors than .click())\n",
    "        next_button = driver.find_element_by_xpath(\"//a[text()='Next']\")\n",
    "        next_button.send_keys(Keys.ENTER)\n",
    "        \n",
    "        ### Wait for the next button to become stale, meaning we've successfully navigated to the next page of results\n",
    "        ### This will throw a TimeoutException on the last page of results, since that page has no next button\n",
    "        ### We'll just catch the exception once, rather than checking the button's existance on every single page\n",
    "        wait.until(EC.staleness_of(next_button))\n",
    "\n",
    "        ### Scroll back to the top so that the first result row is in view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", select_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the WebDriver\n",
    "DRIVER_PATH = 'C:/dev/chromedriver.exe'\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "\n",
    "#Open the results page\n",
    "driver.get('https://www.bmw-berlin-marathon.com/en/impressions/statistics-and-history/results-archive/')\n",
    "\n",
    "#Give the page a second to load\n",
    "time.sleep(1)\n",
    "\n",
    "#Close the cookie modal if it exists\n",
    "try:\n",
    "    driver.find_element_by_xpath(\"//button[@aria-label='Accept all']\").click()\n",
    "except NoSuchElementException:\n",
    "    print('Cookie modal not present')\n",
    "    \n",
    "#Give the page another second, it can be slow sometimes\n",
    "time.sleep(1)\n",
    "    \n",
    "#There is a dropdown that controls which year of results are shown\n",
    "#Get the dropdown, and create a dictionary with each year and the WebDriver element needed to select that year\n",
    "select_element = driver.find_element_by_xpath(f\"//select[@class='events border-input']\")\n",
    "select_options = select_element.find_elements_by_tag_name('option')\n",
    "year_map = dict()\n",
    "for opt in select_options:\n",
    "    year = opt.text[0:4]\n",
    "    year_map[year] = opt\n",
    "    \n",
    "#Create a list to store the results\n",
    "masterResults = []\n",
    "\n",
    "##########################\n",
    "#####   CHOOSE YEAR   ####\n",
    "##########################\n",
    "year = '2009'\n",
    "    \n",
    "#Select the year, then wait for results to load\n",
    "year_map.get(year).click()\n",
    "time.sleep(0.25)\n",
    "\n",
    "#initiatize pagination\n",
    "current_page = 1\n",
    "last_page = find_pagination_limit(driver.page_source)\n",
    "\n",
    "#loop through each page and gather the results\n",
    "num_error = 0\n",
    "while num_error < 5:\n",
    "    \n",
    "    try:\n",
    "        try_do_scrape(current_page, last_page)\n",
    "        \n",
    "    except TimeoutException:\n",
    "        current_page = int(driver.find_element_by_xpath(\"//li[@class='paginate_button page-item active']\").text)\n",
    "        #TimeoutException is thrown on the final page\n",
    "        #If we're on the final page, quit the loop and save\n",
    "        if current_page == last_page:\n",
    "            break\n",
    "            \n",
    "    except (UnexpectedAlertPresentException, StaleElementReferenceException) as ex:\n",
    "        print(f\"**Encountered exception #{num_error+1}**\")\n",
    "        print(ex.Message)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        current_page = int(driver.find_element_by_xpath(\"//li[@class='paginate_button page-item active']\").text)\n",
    "        \n",
    "        \n",
    "        if current_page == last_page:\n",
    "            break\n",
    "        #Go back a page and start over (we'll dedupe during cleanup)\n",
    "        current_page -= 1\n",
    "        prev_button = driver.find_element_by_xpath(\"//a[text()='Previous']\")\n",
    "        prev_button.send_keys(Keys.ENTER)\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", select_element)\n",
    "        num_error += 1\n",
    "    \n",
    "cleanup_and_save(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
